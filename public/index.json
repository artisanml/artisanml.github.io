[{"content":"A tiny framework to perform adversarial validation of your training and test data. Published as a Python package.\nrepository: https://github.com/ilias-ant/adversarial-validation\n","permalink":"http://localhost:1313/projects/open-source/adversarial-validation/","summary":"\u003cp\u003eA tiny framework to perform adversarial validation of your training and test data. Published as a Python package.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003erepository: \u003ca href=\"https://github.com/ilias-ant/adversarial-validation\"\u003ehttps://github.com/ilias-ant/adversarial-validation\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e","title":"adversarial-validation"},{"content":"A Pythonic client for the official data.gov.gr API. Published as a Python package.\nrepository: https://github.com/ilias-ant/pydatagovgr\n","permalink":"http://localhost:1313/projects/open-source/pydatagovgr/","summary":"\u003cp\u003eA Pythonic client for the official data.gov.gr API. Published as a Python package.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003erepository: \u003ca href=\"https://github.com/ilias-ant/pydatagovgr\"\u003ehttps://github.com/ilias-ant/pydatagovgr\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e","title":"pydatagovgr"},{"content":"Want to get notified on the progress of your TensorFlow model training? This is a collection of robust tf.keras callbacks to send notifications to a messaging app of your choice. Published as a Python package.\nrepository: https://github.com/ilias-ant/tf-notify\n","permalink":"http://localhost:1313/projects/open-source/tf-notify/","summary":"\u003cp\u003eWant to get notified on the progress of your TensorFlow model training? This is a collection of robust \u003ccode\u003etf.keras\u003c/code\u003e\ncallbacks to send notifications to a messaging app of your choice. Published as a Python package.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003erepository: \u003ca href=\"https://github.com/ilias-ant/tf-notify\"\u003ehttps://github.com/ilias-ant/tf-notify\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e","title":"tf-notify"},{"content":"An app that scrapes public articles from the American Alpine Club\u0026rsquo;s publications: AAJ and ANAC.\nThis work has been published as a Kaggle dataset.\nrepository: https://github.com/ilias-ant/american-alpine-club-articles\n","permalink":"http://localhost:1313/projects/open-source/american-alpine-club-articles/","summary":"\u003cp\u003eAn app that scrapes public articles from the American Alpine Club\u0026rsquo;s publications: AAJ and ANAC.\u003c/p\u003e\n\u003cp\u003eThis work has been published as a \u003ca href=\"https://www.kaggle.com/datasets/iantonopoulos/american-alpine-club-articles\"\u003eKaggle dataset\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003erepository: \u003ca href=\"https://github.com/ilias-ant/american-alpine-club-articles\"\u003ehttps://github.com/ilias-ant/american-alpine-club-articles\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e","title":"american-alpine-club-articles"},{"content":"A library to easily convert rock climbing route grades between different grading systems. Published as a Python package.\nrepository: https://github.com/ilias-ant/pyclimb\n","permalink":"http://localhost:1313/projects/open-source/pyclimb/","summary":"\u003cp\u003eA library to easily convert rock climbing route grades between different grading systems. Published as a Python package.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003erepository: \u003ca href=\"https://github.com/ilias-ant/pyclimb\"\u003ehttps://github.com/ilias-ant/pyclimb\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e","title":"pyclimb"},{"content":"Having worked in the industry for 6 years now, I\u0026rsquo;ve seen machine learning projects succeed but, most importantly, I\u0026rsquo;ve seen many more fail - I even failed some of them myself due to inexperience and/or poor judgement. Although each failure has its own story, reasons and learnings, some common denominators always exist.\nOne such important denominator can be \u0026ldquo;boiled down\u0026rdquo; to a single assertion that - as an ML engineer - I am particularly fond of:\n\u0026ldquo;Chances are, you do NOT need ML to solve THAT problem.\u0026rdquo;\nI get it - building a machine learning model is exciting stuff. Observing it as it generates predictions on your company\u0026rsquo;s production environment is even more exciting.\nBut, there\u0026rsquo;s \u0026ldquo;no free lunch\u0026rdquo;.\nBeing a good engineer in the industry is typically not about building complex, state-of-the-art sophisticated solutions - this is not academia. It\u0026rsquo;s all about generating constant and considerable value for the product and company that you work for, while being cost-effective. To do that, you should be dead certain which is the right tool and approach for each problem that you\u0026rsquo;re required to solve. ML is just one of many tools in an engineer\u0026rsquo;s toolbox.\nCoincidentally, the last couple of weeks I have been reading Chip Huyen\u0026rsquo;s great book called Designing Machine Learning Systems. This realization, that ML is not always the right tool for the job, is one of the main takeaways from the book.\nChip went a step further and created a list of 9 criteria that we should consider before commiting to an ML solution for our problem. I found this list to be very accurate \u0026amp; useful, so decided to share it here. ML is the right approach when:\nthe system has the capacity to learn. there are patterns to learn, and they are complex. data are available, or it\u0026rsquo;s possible to collect data. it\u0026rsquo;s a predictive problem. unseen data shares patterns with the training data. it\u0026rsquo;s repetitive. the cost of wrong predictions it\u0026rsquo;s cheap. it\u0026rsquo;s at scale. the patterns are constantly changing. Next time you\u0026rsquo;re faced with a problem that you think ML might be the right tool for, consider these 9 criteria. Go through each one of them and see if they apply to your problem. Make sure that all of them are met before you commit to an ML solution. If not, chances are that there exists a simpler, more cost-effective solution for this problem. Take some time to think what this solution might be.\n","permalink":"http://localhost:1313/blog/do-we-even-need-ml/","summary":"\u003cp\u003eHaving worked in the industry for 6 years now, I\u0026rsquo;ve seen machine learning projects succeed but, most\nimportantly, I\u0026rsquo;ve seen many more fail - I even failed some of them myself due to inexperience and/or poor judgement.\nAlthough each failure has its own story, reasons and learnings, some common denominators always exist.\u003c/p\u003e\n\u003cp\u003eOne such important denominator can be \u0026ldquo;boiled down\u0026rdquo; to a single assertion that - as an ML engineer - I am particularly\nfond of:\u003c/p\u003e","title":"Do we even need ML?"},{"content":"the problem A common workflow in machine learning projects (especially in Kaggle competitions) is:\ntrain your ML model in a training dataset. tune and validate your ML model in a validation dataset (typically is a discrete fraction of the training dataset). finally, assess the actual generalization ability of your ML model in a \u0026ldquo;held-out\u0026rdquo; test dataset. This strategy is widely accepted, as it forces the practitioner to interact with the ever important test dataset only once, at the end of the model selection process - and purely for performance assessment purposes. Any feedback derived from the evaluation on the test dataset does not influence the model selection process further, thus preventing overfitting.\nHowever, the success of this strategy heavily relies on the following assumption:\n\u0026ldquo;The training and test datasets are drawn from the same underlying distribution.\u0026rdquo;\nThis is often referred to as the \u0026ldquo;identically distributed\u0026rdquo; property in the literature, and - more importantly - it may not hold true.\nIt is probable for a test dataset to have a slightly different distribution than the training dataset. This introduces a logical fallacy in the step #2 of the workflow described above: the validation dataset (sharing the same distributional properties with the training dataset) is used to validate the model, which is - in this case - not a very good proxy for the performance on the unseen test instances.\nIn other words, you might end up with a very satisfactory model performance based the validation dataset, only to be disappointed by the actual performance of the model on the test dataset.\nthe solution So, what can we do if the \u0026ldquo;identically distributed\u0026rdquo; property does not actually hold, but we are not aware of it?\nEnter, adversarial validation.\nIt\u0026rsquo;s actually a flamboyant name for a simple, yet clever idea:\nLet\u0026rsquo;s assume that the \u0026ldquo;identically distributed\u0026rdquo; property holds i.e. the training and test datasets are drawn from the same underlying distribution. In this case, if we were to train a binary classifier on the task of distinguishing instances that belong to the test dataset from those that belong to the training dataset, we would expect that this classifier would perform no better than random.\nThis \u0026ldquo;no better than random\u0026rdquo; performance can be easily quantified through the Area Under the ROC metric. Such a classifier (performing no better than random in this specific task) is bound to have an approximate 0.5 ROC AUC score.\nIf, on the other hand, the \u0026ldquo;identically distributed\u0026rdquo; property does not hold, then the classifier will be able to distinguish between training \u0026amp; test instances, so naturally a ROC AUC score significantly higher than 0.5 is expected.\nNow, let\u0026rsquo;s see all these unfold in action:\nexamples We will present two different use cases (straight out of Kaggle competitions) that vividly highlight the concept of adversarial validation: in the first one, the \u0026ldquo;identically distributed\u0026rdquo; property holds, while in the second one it does not.\ndata where the \u0026ldquo;identically distributed\u0026rdquo; property holds The first use case is derived from the House Prices: Advanced Regression Techniques Kaggle competition.\ntrain = pd.read_csv(\u0026#34;train.csv\u0026#34;) test = pd.read_csv(\u0026#34;test.csv\u0026#34;) We will filter-out all the non-numeric columns, just to keep things simple:\nX_train = train.select_dtypes(include=np.number).copy() X_test = test.select_dtypes(include=np.number).copy() After a quick review of the training dataset and the competition description, we can identify the target variable, which is the SalePrice column. Let\u0026rsquo;s drop it from the training dataset.\nX_train = X_train.drop([\u0026#34;SalePrice\u0026#34;], axis=1) Now, we can introduce a new artificial target variable in order to train a binary classifier that will try to distiguish between training and test instances.\n# you can call this new target column whatever you like X_train[\u0026#34;_adval_label_\u0026#34;] = 0.0 X_test[\u0026#34;_adval_label_\u0026#34;] = 1.0 In the remaining preprocessing steps, we will combine together the training and test datasets, shuffle them real good and finally create a new design matrix X and a new target vector y:\ncombined_dataset = pd.concat([X_train, X_test], axis=0, ignore_index=True) combined_dataset_shuffled = combined_dataset.sample(frac=1) X = combined_dataset_shuffled.drop([\u0026#34;_adval_label_\u0026#34;], axis=1) y = combined_dataset_shuffled[\u0026#34;_adval_label_\u0026#34;] Now, it\u0026rsquo;s time to train a classifier for this task:\nn_splits = 5 classifier = xgb.XGBClassifier() cv = StratifiedKFold(n_splits=n_splits) ... The classifier of choice is the XGBoost classifier, for which we will use a typical stratified 5-fold cross-validation scheme. At each fold, we will calculate the ROC AUC score. At the end of this process, we get the following ROC AUC curve:\nwith individual per-fold scores:\n\u0026gt;\u0026gt;\u0026gt; [1.0, 0.9982876712328766, 1.0, 1.0, 1.0] Oops. It seems that our classifier - despite our strategy - managed to distinguish almost perfectly between training and test instances. Something is not right here. Let\u0026rsquo;s take a look at feature importance:\nclassifier = xgb.XGBClassifier(eval_metric=\u0026#39;logloss\u0026#39;) classifier.fit(X, y) fig, ax = plt.subplots(figsize=(12,4)) xgb.plot_importance(classifier, ax=ax) plt.show(); Something nasty is happening with the Id column - let\u0026rsquo;s go back to the raw data:\nIt is apparent that our classifier can easily distinguish training from test instances via a very simple heuristic: Id \u0026gt;= 1461. Totally different distributions between training \u0026amp; test datasets. So, let\u0026rsquo;s drop this column and try again:\nwith individual per-fold scores:\n\u0026gt;\u0026gt;\u0026gt; [0.5219787952711578, 0.5326398010883844, 0.5170529179958716, 0.5292620566710452, 0.5295862166360683] Much better. This is the result we were looking for: our classifier has now lost the ability to distinguish between training and test datasets yielding a mean ROC AUC score of 0.5148.\nSo, training \u0026amp; test datasets are really drawn from the same underlying distribution - assuming that the highly problematicId column is not part of the design matrix.\ndata where the \u0026ldquo;identically distributed\u0026rdquo; property does not hold The second use case is derived from the Sberbank Russian Housing Market Kaggle competition.\ntrainset = pd.read_csv(\u0026#34;data/sberbank-russian-housing-market/train.csv\u0026#34;) testset = pd.read_csv(\u0026#34;data/sberbank-russian-housing-market/test.csv\u0026#34;) Target variable here is price_doc:\nX_train = trainset.drop([\u0026#34;price_doc\u0026#34;], axis=1) Similarly to the Id situation of the previous example, here we have columns id and timestamp that we can drop from the design matrix, since they are features with highly identifiable properties (having monotonically increasing values).\nThen following roughly the same pre-processing logic and applying a 5-fold cross-validation scheme as previously, we get:\nwith individual per-fold scores:\n\u0026gt;\u0026gt;\u0026gt; [0.9930616257454692, 0.9933295525996183, 0.993228891121216, 0.9927296013456716, 0.993396535221478] Even with the absence of id and timestamp, our classifier was able to almost perfectly distinguish between training and test instances. This implies significant difference in the underlying distributions of the two datasets, so using a validation dataset to derive a proxy performance for the unseen test instances is bound to yield disappointment.\nAnd now, the most important question of all.\nis there any hope? Is there anything we can do when out training and test datasets differ? Can we generate a reasonably good validation dataset that we can trust?\nIntuitively, we cannot do something to \u0026ldquo;bring\u0026rdquo; the training dataset \u0026ldquo;closer\u0026rdquo; to the test dataset. Our best hope is to select the validation dataset (out of the training dataset) in a way that best resembles the test dataset i.e. select instances out of the training dataset which are most \u0026ldquo;similar\u0026rdquo; to the test dataset. This solution has been proposed by Zygmunt Zając (see related reading at the end).\nThis seems sensible - but how?\nOur \u0026ldquo;adversarial\u0026rdquo; classifier can help us here, again. The plan is to:\nyield predictions for all training instances. see which training instances are misclassified as test instances. just use those to form the validation dataset. Regarding step #1, a simple cross-validation scheme is a suitable approach to get predictions for all training instances.\nNow onto step #2: we would like to get all the training instances that were misclassified with high probability i.e. the classifier mislabelled as test instances (although they were training ones) and it was quite certain about it.\npredictions = [] for fold, (train, test) in tqdm([*enumerate(cv.split(X, y))]): classifier.fit(X.iloc[train], y.iloc[train]) y_pred = classifier.predict_proba(X.iloc[test])[:, 1] predictions.extend(y_pred.tolist()) combined_dataset[\u0026#34;preds\u0026#34;] = predictions trainset = combined_dataset[combined_dataset[\u0026#34;_adval_label_\u0026#34;] == 0.0] i = trainset[\u0026#34;preds\u0026#34;].argsort() sorted_trainset = trainset.iloc[i] That\u0026rsquo;s about it: sorted_trainset contains all the training instances, sorted from lowest to highest probability of instance being a test instance. \u0026ldquo;Slicing\u0026rdquo; from the bottom of the table will get you the training instances that are intuitively more similar to the test dataset, thus good candidates for the validation dataset.\nFor example:\ntrainset[trainset[\u0026#34;preds\u0026#34;] \u0026gt;= 0.5] yields the few misclassified training instances.\na birds-eye view To wrap all these up, let\u0026rsquo;s take a step back and look at the adversarial validation mechanism again - this time in a bit more abstract way.\nLet\u0026rsquo;s imagine some typical training and test datasets, described by N numerical features and a target variable. The following flowchart acts as a visual representation of the adversarial validation mechanism.\nas a Python package Based on the generality of the adversarial validation mechanism above, i searched Github.com for a pip that implements this as a Python package. I didn\u0026rsquo;t find anything worth considering, so i decided to develop and publish one!\nI named it advertion, which stands for adversarial validation):\nhttps://github.com/ilias-ant/adversarial-validation\nconclusion We went through the concept of adversarial validation, a simple technique that helps you assert whether training \u0026amp; test instances follow the same underlying distribution through a classification scheme: membership in test dataset is used as a binary target variable upon which a classifier tries to distinguish between the two, trained on a combined, shuffled dataset.\nWe then saw two practical examples from Kaggle competitions, covering both cases. In the first example, we saw that the classifier could not distinguish between training \u0026amp; test instances, thus training \u0026amp; test datasets must have the same distributions. In the second example, we saw quite the opposite.\nAfter that, we tried to answer the most crucial question: what can we do in case our datasets differ? and provided a simple solution.\nLastly, wrapped up the article with a birds-eye view on the adversarial validation mechanism and highlighted an open-source Python package that basically offers this validation through a simple API.\nrelated reading The following are, to the best of my knowledge, the first mention of the adversarial validation concept and the proposed solution - definitely a good read:\nAdversarial validation, part one by Zygmunt Zając Adversarial validation, part two by Zygmunt Zając W.r.t. code:\nCode for the examples above: https://github.com/ilias-ant/adversarial-validation-demo The open-source Python package: https://github.com/ilias-ant/adversarial-validation The documentation for the package: https://advertion.readthedocs.io/en/latest/ ","permalink":"http://localhost:1313/blog/adversarial-validation/","summary":"\u003ch2 id=\"the-problem\"\u003ethe problem\u003c/h2\u003e\n\u003cp\u003eA common workflow in machine learning projects (especially in \u003ca href=\"https://www.kaggle.com/\"\u003eKaggle\u003c/a\u003e competitions) is:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003etrain your ML model in a training dataset.\u003c/li\u003e\n\u003cli\u003etune and validate your ML model in a validation dataset (\u003cem\u003etypically is a discrete fraction of the training\ndataset\u003c/em\u003e).\u003c/li\u003e\n\u003cli\u003efinally, assess the actual generalization ability of your ML model in a \u0026ldquo;held-out\u0026rdquo; test dataset.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis strategy is widely accepted, as it forces the practitioner to  interact with the ever important test dataset\nonly once, at the end of the model selection process - and purely for performance assessment purposes. Any feedback\nderived from the evaluation on the test dataset does not influence the model selection process further, thus preventing\noverfitting.\u003c/p\u003e","title":"Adversarial Validation: can i trust my validation dataset?"}]