<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Adversarial Validation: can i trust my validation dataset? | ARTISAN ML</title>
<meta name="keywords" content="adversarial-validation">
<meta name="description" content="the problem
A common workflow in machine learning projects (especially in Kaggle competitions) is:

train your ML model in a training dataset.
tune and validate your ML model in a validation dataset (typically is a discrete fraction of the training
dataset).
finally, assess the actual generalization ability of your ML model in a &ldquo;held-out&rdquo; test dataset.

This strategy is widely accepted, as it forces the practitioner to  interact with the ever important test dataset
only once, at the end of the model selection process - and purely for performance assessment purposes. Any feedback
derived from the evaluation on the test dataset does not influence the model selection process further, thus preventing
overfitting.">
<meta name="author" content="Ilias Antonopoulos">
<link rel="canonical" href="http://localhost:1313/blog/adversarial-validation/">
<link crossorigin="anonymous" href="../../assets/css/stylesheet.f60300bd1212618437b2be817e250a43b03a0c55846a3b5c39869afbb7e9ed81.css" integrity="sha256-9gMAvRISYYQ3sr6BfiUKQ7A6DFWEajtcOYaa&#43;7fp7YE=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="../../assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="http://localhost:1313/favicons/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicons/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicons/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/favicons/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blog/adversarial-validation/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Adversarial Validation: can i trust my validation dataset?" />
<meta property="og:description" content="the problem
A common workflow in machine learning projects (especially in Kaggle competitions) is:

train your ML model in a training dataset.
tune and validate your ML model in a validation dataset (typically is a discrete fraction of the training
dataset).
finally, assess the actual generalization ability of your ML model in a &ldquo;held-out&rdquo; test dataset.

This strategy is widely accepted, as it forces the practitioner to  interact with the ever important test dataset
only once, at the end of the model selection process - and purely for performance assessment purposes. Any feedback
derived from the evaluation on the test dataset does not influence the model selection process further, thus preventing
overfitting." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/blog/adversarial-validation/" />
<meta property="og:image" content="http://localhost:1313/blog-posts/adversarial-validation/cover.png" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2023-07-22T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-07-22T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="http://localhost:1313/blog-posts/adversarial-validation/cover.png" />
<meta name="twitter:title" content="Adversarial Validation: can i trust my validation dataset?"/>
<meta name="twitter:description" content="the problem
A common workflow in machine learning projects (especially in Kaggle competitions) is:

train your ML model in a training dataset.
tune and validate your ML model in a validation dataset (typically is a discrete fraction of the training
dataset).
finally, assess the actual generalization ability of your ML model in a &ldquo;held-out&rdquo; test dataset.

This strategy is widely accepted, as it forces the practitioner to  interact with the ever important test dataset
only once, at the end of the model selection process - and purely for performance assessment purposes. Any feedback
derived from the evaluation on the test dataset does not influence the model selection process further, thus preventing
overfitting."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "",
      "item": "http://localhost:1313/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Adversarial Validation: can i trust my validation dataset?",
      "item": "http://localhost:1313/blog/adversarial-validation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Adversarial Validation: can i trust my validation dataset?",
  "name": "Adversarial Validation: can i trust my validation dataset?",
  "description": "the problem A common workflow in machine learning projects (especially in Kaggle competitions) is:\ntrain your ML model in a training dataset. tune and validate your ML model in a validation dataset (typically is a discrete fraction of the training dataset). finally, assess the actual generalization ability of your ML model in a \u0026ldquo;held-out\u0026rdquo; test dataset. This strategy is widely accepted, as it forces the practitioner to interact with the ever important test dataset only once, at the end of the model selection process - and purely for performance assessment purposes. Any feedback derived from the evaluation on the test dataset does not influence the model selection process further, thus preventing overfitting.\n",
  "keywords": [
    "adversarial-validation"
  ],
  "articleBody": "the problem A common workflow in machine learning projects (especially in Kaggle competitions) is:\ntrain your ML model in a training dataset. tune and validate your ML model in a validation dataset (typically is a discrete fraction of the training dataset). finally, assess the actual generalization ability of your ML model in a “held-out” test dataset. This strategy is widely accepted, as it forces the practitioner to interact with the ever important test dataset only once, at the end of the model selection process - and purely for performance assessment purposes. Any feedback derived from the evaluation on the test dataset does not influence the model selection process further, thus preventing overfitting.\nHowever, the success of this strategy heavily relies on the following assumption:\n“The training and test datasets are drawn from the same underlying distribution.”\nThis is often referred to as the “identically distributed” property in the literature, and - more importantly - it may not hold true.\nIt is probable for a test dataset to have a slightly different distribution than the training dataset. This introduces a logical fallacy in the step #2 of the workflow described above: the validation dataset (sharing the same distributional properties with the training dataset) is used to validate the model, which is - in this case - not a very good proxy for the performance on the unseen test instances.\nIn other words, you might end up with a very satisfactory model performance based the validation dataset, only to be disappointed by the actual performance of the model on the test dataset.\nthe solution So, what can we do if the “identically distributed” property does not actually hold, but we are not aware of it?\nEnter, adversarial validation.\nIt’s actually a flamboyant name for a simple, yet clever idea:\nLet’s assume that the “identically distributed” property holds i.e. the training and test datasets are drawn from the same underlying distribution. In this case, if we were to train a binary classifier on the task of distinguishing instances that belong to the test dataset from those that belong to the training dataset, we would expect that this classifier would perform no better than random.\nThis “no better than random” performance can be easily quantified through the Area Under the ROC metric. Such a classifier (performing no better than random in this specific task) is bound to have an approximate 0.5 ROC AUC score.\nIf, on the other hand, the “identically distributed” property does not hold, then the classifier will be able to distinguish between training \u0026 test instances, so naturally a ROC AUC score significantly higher than 0.5 is expected.\nNow, let’s see all these unfold in action:\nexamples We will present two different use cases (straight out of Kaggle competitions) that vividly highlight the concept of adversarial validation: in the first one, the “identically distributed” property holds, while in the second one it does not.\ndata where the “identically distributed” property holds The first use case is derived from the House Prices: Advanced Regression Techniques Kaggle competition.\ntrain = pd.read_csv(\"train.csv\") test = pd.read_csv(\"test.csv\") We will filter-out all the non-numeric columns, just to keep things simple:\nX_train = train.select_dtypes(include=np.number).copy() X_test = test.select_dtypes(include=np.number).copy() After a quick review of the training dataset and the competition description, we can identify the target variable, which is the SalePrice column. Let’s drop it from the training dataset.\nX_train = X_train.drop([\"SalePrice\"], axis=1) Now, we can introduce a new artificial target variable in order to train a binary classifier that will try to distiguish between training and test instances.\n# you can call this new target column whatever you like X_train[\"_adval_label_\"] = 0.0 X_test[\"_adval_label_\"] = 1.0 In the remaining preprocessing steps, we will combine together the training and test datasets, shuffle them real good and finally create a new design matrix X and a new target vector y:\ncombined_dataset = pd.concat([X_train, X_test], axis=0, ignore_index=True) combined_dataset_shuffled = combined_dataset.sample(frac=1) X = combined_dataset_shuffled.drop([\"_adval_label_\"], axis=1) y = combined_dataset_shuffled[\"_adval_label_\"] Now, it’s time to train a classifier for this task:\nn_splits = 5 classifier = xgb.XGBClassifier() cv = StratifiedKFold(n_splits=n_splits) ... The classifier of choice is the XGBoost classifier, for which we will use a typical stratified 5-fold cross-validation scheme. At each fold, we will calculate the ROC AUC score. At the end of this process, we get the following ROC AUC curve:\nwith individual per-fold scores:\n\u003e\u003e\u003e [1.0, 0.9982876712328766, 1.0, 1.0, 1.0] Oops. It seems that our classifier - despite our strategy - managed to distinguish almost perfectly between training and test instances. Something is not right here. Let’s take a look at feature importance:\nclassifier = xgb.XGBClassifier(eval_metric='logloss') classifier.fit(X, y) fig, ax = plt.subplots(figsize=(12,4)) xgb.plot_importance(classifier, ax=ax) plt.show(); Something nasty is happening with the Id column - let’s go back to the raw data:\nIt is apparent that our classifier can easily distinguish training from test instances via a very simple heuristic: Id \u003e= 1461. Totally different distributions between training \u0026 test datasets. So, let’s drop this column and try again:\nwith individual per-fold scores:\n\u003e\u003e\u003e [0.5219787952711578, 0.5326398010883844, 0.5170529179958716, 0.5292620566710452, 0.5295862166360683] Much better. This is the result we were looking for: our classifier has now lost the ability to distinguish between training and test datasets yielding a mean ROC AUC score of 0.5148.\nSo, training \u0026 test datasets are really drawn from the same underlying distribution - assuming that the highly problematicId column is not part of the design matrix.\ndata where the “identically distributed” property does not hold The second use case is derived from the Sberbank Russian Housing Market Kaggle competition.\ntrainset = pd.read_csv(\"data/sberbank-russian-housing-market/train.csv\") testset = pd.read_csv(\"data/sberbank-russian-housing-market/test.csv\") Target variable here is price_doc:\nX_train = trainset.drop([\"price_doc\"], axis=1) Similarly to the Id situation of the previous example, here we have columns id and timestamp that we can drop from the design matrix, since they are features with highly identifiable properties (having monotonically increasing values).\nThen following roughly the same pre-processing logic and applying a 5-fold cross-validation scheme as previously, we get:\nwith individual per-fold scores:\n\u003e\u003e\u003e [0.9930616257454692, 0.9933295525996183, 0.993228891121216, 0.9927296013456716, 0.993396535221478] Even with the absence of id and timestamp, our classifier was able to almost perfectly distinguish between training and test instances. This implies significant difference in the underlying distributions of the two datasets, so using a validation dataset to derive a proxy performance for the unseen test instances is bound to yield disappointment.\nAnd now, the most important question of all.\nis there any hope? Is there anything we can do when out training and test datasets differ? Can we generate a reasonably good validation dataset that we can trust?\nIntuitively, we cannot do something to “bring” the training dataset “closer” to the test dataset. Our best hope is to select the validation dataset (out of the training dataset) in a way that best resembles the test dataset i.e. select instances out of the training dataset which are most “similar” to the test dataset. This solution has been proposed by Zygmunt Zając (see related reading at the end).\nThis seems sensible - but how?\nOur “adversarial” classifier can help us here, again. The plan is to:\nyield predictions for all training instances. see which training instances are misclassified as test instances. just use those to form the validation dataset. Regarding step #1, a simple cross-validation scheme is a suitable approach to get predictions for all training instances.\nNow onto step #2: we would like to get all the training instances that were misclassified with high probability i.e. the classifier mislabelled as test instances (although they were training ones) and it was quite certain about it.\npredictions = [] for fold, (train, test) in tqdm([*enumerate(cv.split(X, y))]): classifier.fit(X.iloc[train], y.iloc[train]) y_pred = classifier.predict_proba(X.iloc[test])[:, 1] predictions.extend(y_pred.tolist()) combined_dataset[\"preds\"] = predictions trainset = combined_dataset[combined_dataset[\"_adval_label_\"] == 0.0] i = trainset[\"preds\"].argsort() sorted_trainset = trainset.iloc[i] That’s about it: sorted_trainset contains all the training instances, sorted from lowest to highest probability of instance being a test instance. “Slicing” from the bottom of the table will get you the training instances that are intuitively more similar to the test dataset, thus good candidates for the validation dataset.\nFor example:\ntrainset[trainset[\"preds\"] \u003e= 0.5] yields the few misclassified training instances.\na birds-eye view To wrap all these up, let’s take a step back and look at the adversarial validation mechanism again - this time in a bit more abstract way.\nLet’s imagine some typical training and test datasets, described by N numerical features and a target variable. The following flowchart acts as a visual representation of the adversarial validation mechanism.\nas a Python package Based on the generality of the adversarial validation mechanism above, i searched Github.com for a pip that implements this as a Python package. I didn’t find anything worth considering, so i decided to develop and publish one!\nI named it advertion, which stands for adversarial validation):\nhttps://github.com/ilias-ant/adversarial-validation\nconclusion We went through the concept of adversarial validation, a simple technique that helps you assert whether training \u0026 test instances follow the same underlying distribution through a classification scheme: membership in test dataset is used as a binary target variable upon which a classifier tries to distinguish between the two, trained on a combined, shuffled dataset.\nWe then saw two practical examples from Kaggle competitions, covering both cases. In the first example, we saw that the classifier could not distinguish between training \u0026 test instances, thus training \u0026 test datasets must have the same distributions. In the second example, we saw quite the opposite.\nAfter that, we tried to answer the most crucial question: what can we do in case our datasets differ? and provided a simple solution.\nLastly, wrapped up the article with a birds-eye view on the adversarial validation mechanism and highlighted an open-source Python package that basically offers this validation through a simple API.\nrelated reading The following are, to the best of my knowledge, the first mention of the adversarial validation concept and the proposed solution - definitely a good read:\nAdversarial validation, part one by Zygmunt Zając Adversarial validation, part two by Zygmunt Zając W.r.t. code:\nCode for the examples above: https://github.com/ilias-ant/adversarial-validation-demo The open-source Python package: https://github.com/ilias-ant/adversarial-validation The documentation for the package: https://advertion.readthedocs.io/en/latest/ ",
  "wordCount" : "1671",
  "inLanguage": "en",
  "image":"http://localhost:1313/blog-posts/adversarial-validation/cover.png","datePublished": "2023-07-22T00:00:00Z",
  "dateModified": "2023-07-22T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Ilias Antonopoulos"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blog/adversarial-validation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "ARTISAN ML",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicons/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top"><head>
    
</head>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="ARTISAN ML (Alt + H)">ARTISAN ML</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="home">
                    <span>home</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blog/"></a></div>
    <h1 class="post-title">
      Adversarial Validation: can i trust my validation dataset?
    </h1>
    <div class="post-meta">&lt;span title=&#39;2023-07-22 00:00:00 &#43;0000 UTC&#39;&gt;July 22, 2023&lt;/span&gt;&amp;nbsp;·&amp;nbsp;8 min&amp;nbsp;·&amp;nbsp;Ilias Antonopoulos

</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="http://localhost:1313/blog-posts/adversarial-validation/cover.png" alt="counting">
        <p>training &amp; test datasets are not always drawn from the same distribution.</p>
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#the-problem" aria-label="the problem">the problem</a></li>
                <li>
                    <a href="#the-solution" aria-label="the solution">the solution</a></li>
                <li>
                    <a href="#examples" aria-label="examples">examples</a><ul>
                        
                <li>
                    <a href="#data-where-the-identically-distributed-property-holds" aria-label="data where the &ldquo;identically distributed&rdquo; property holds">data where the &ldquo;identically distributed&rdquo; property holds</a></li>
                <li>
                    <a href="#data-where-the-identically-distributed-property-does-not-hold" aria-label="data where the &ldquo;identically distributed&rdquo; property does not hold">data where the &ldquo;identically distributed&rdquo; property does not hold</a></li></ul>
                </li>
                <li>
                    <a href="#is-there-any-hope" aria-label="is there any hope?">is there any hope?</a></li>
                <li>
                    <a href="#a-birds-eye-view" aria-label="a birds-eye view">a birds-eye view</a></li>
                <li>
                    <a href="#as-a-python-package" aria-label="as a Python package">as a Python package</a></li>
                <li>
                    <a href="#conclusion" aria-label="conclusion">conclusion</a></li>
                <li>
                    <a href="#related-reading" aria-label="related reading">related reading</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="the-problem">the problem<a hidden class="anchor" aria-hidden="true" href="#the-problem">#</a></h2>
<p>A common workflow in machine learning projects (especially in <a href="https://www.kaggle.com/">Kaggle</a> competitions) is:</p>
<ol>
<li>train your ML model in a training dataset.</li>
<li>tune and validate your ML model in a validation dataset (<em>typically is a discrete fraction of the training
dataset</em>).</li>
<li>finally, assess the actual generalization ability of your ML model in a &ldquo;held-out&rdquo; test dataset.</li>
</ol>
<p>This strategy is widely accepted, as it forces the practitioner to  interact with the ever important test dataset
only once, at the end of the model selection process - and purely for performance assessment purposes. Any feedback
derived from the evaluation on the test dataset does not influence the model selection process further, thus preventing
overfitting.</p>
<p>However, the success of this strategy heavily relies on the following assumption:</p>
<blockquote>
<p>&ldquo;The training and test datasets are drawn from the same underlying distribution.&rdquo;</p></blockquote>
<p>This is often referred to as the &ldquo;<em>identically distributed</em>&rdquo; property in the literature, and - more importantly - it may
not hold true.</p>
<p>It is probable for a test dataset to have a slightly different <a href="https://en.wikipedia.org/wiki/Probability_distribution">distribution</a>
than the training dataset. This introduces a logical fallacy in the step #2 of the workflow described above: the
validation dataset (<em>sharing the same distributional properties with the training dataset</em>) is used to validate the
model, which is - in this case - not a very good proxy for the performance on the unseen test instances.</p>
<p>In other words, you might end up with a very satisfactory model performance based the validation dataset, only to be
disappointed by the actual performance of the model on the test dataset.</p>
<h2 id="the-solution">the solution<a hidden class="anchor" aria-hidden="true" href="#the-solution">#</a></h2>
<p>So, what can we do if the &ldquo;<em>identically distributed</em>&rdquo; property does not actually hold, but we are not aware of it?</p>
<p>Enter, <strong>adversarial validation</strong>.</p>
<p>It&rsquo;s actually a flamboyant name for a simple, yet clever idea:</p>
<p>Let&rsquo;s assume that the &ldquo;<em>identically distributed</em>&rdquo; property holds i.e. the training and test datasets are drawn from the
same underlying distribution. In this case, if we were to train a binary classifier on the task of distinguishing
instances that belong to the test dataset from those that belong to the training dataset, we would expect that this
classifier would perform no better than random.</p>
<p>This &ldquo;no better than random&rdquo; performance can be easily quantified through the
<a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve">Area Under the ROC</a> metric. Such
a classifier (<em>performing no better than random in this specific task</em>) is bound to have an approximate <code>0.5</code> ROC AUC
score.</p>
<p>If, on the other hand, the &ldquo;<em>identically distributed</em>&rdquo; property does not hold, then the classifier will be able to
distinguish between training &amp; test instances, so naturally a ROC AUC score significantly higher than <code>0.5</code> is expected.</p>
<p>Now, let&rsquo;s see all these unfold in action:</p>
<h2 id="examples">examples<a hidden class="anchor" aria-hidden="true" href="#examples">#</a></h2>
<p>We will present two different use cases (straight out of Kaggle competitions) that vividly highlight the concept of
adversarial validation: in the first one, the &ldquo;<em>identically distributed</em>&rdquo; property holds, while in the second one it
does not.</p>
<h3 id="data-where-the-identically-distributed-property-holds">data where the &ldquo;identically distributed&rdquo; property holds<a hidden class="anchor" aria-hidden="true" href="#data-where-the-identically-distributed-property-holds">#</a></h3>
<p>The first use case is derived from the
<a href="https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a>
Kaggle competition.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>train <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#34;train.csv&#34;</span>)
</span></span><span style="display:flex;"><span>test <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#34;test.csv&#34;</span>)
</span></span></code></pre></div><p>We will filter-out all the non-numeric columns, just to keep things simple:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X_train <span style="color:#f92672">=</span> train<span style="color:#f92672">.</span>select_dtypes(include<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>number)<span style="color:#f92672">.</span>copy()
</span></span><span style="display:flex;"><span>X_test <span style="color:#f92672">=</span> test<span style="color:#f92672">.</span>select_dtypes(include<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>number)<span style="color:#f92672">.</span>copy()
</span></span></code></pre></div><p>After a quick review of the training dataset and the competition description, we can identify the target variable,
which is the <code>SalePrice</code> column. Let&rsquo;s drop it from the training dataset.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X_train <span style="color:#f92672">=</span> X_train<span style="color:#f92672">.</span>drop([<span style="color:#e6db74">&#34;SalePrice&#34;</span>], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>Now, we can introduce a new artificial target variable in order to train a binary classifier that will try to
distiguish between training and test instances.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># you can call this new target column whatever you like</span>
</span></span><span style="display:flex;"><span>X_train[<span style="color:#e6db74">&#34;_adval_label_&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>X_test[<span style="color:#e6db74">&#34;_adval_label_&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
</span></span></code></pre></div><p>In the remaining preprocessing steps, we will combine together the training and test datasets, shuffle them real good
and finally create a new design matrix <code>X</code> and a new target vector <code>y</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>combined_dataset <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>concat([X_train, X_test], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, ignore_index<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>combined_dataset_shuffled <span style="color:#f92672">=</span> combined_dataset<span style="color:#f92672">.</span>sample(frac<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> combined_dataset_shuffled<span style="color:#f92672">.</span>drop([<span style="color:#e6db74">&#34;_adval_label_&#34;</span>], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> combined_dataset_shuffled[<span style="color:#e6db74">&#34;_adval_label_&#34;</span>]
</span></span></code></pre></div><p>Now, it&rsquo;s time to train a classifier for this task:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>n_splits <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>classifier <span style="color:#f92672">=</span> xgb<span style="color:#f92672">.</span>XGBClassifier()
</span></span><span style="display:flex;"><span>cv <span style="color:#f92672">=</span> StratifiedKFold(n_splits<span style="color:#f92672">=</span>n_splits)
</span></span><span style="display:flex;"><span><span style="color:#f92672">...</span>
</span></span></code></pre></div><p>The classifier of choice is the XGBoost classifier, for which we will use a typical stratified 5-fold cross-validation
scheme. At each fold, we will calculate the ROC AUC score. At the end of this process, we get the following ROC AUC
curve:</p>
<p><img loading="lazy" src="../../blog-posts/adversarial-validation/roc-auc-curve-1.png" alt="roc-auc-curve-1"  />
</p>
<p>with individual per-fold scores:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;&gt;</span> [<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">0.9982876712328766</span>, <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">1.0</span>]
</span></span></code></pre></div><p><strong>Oops.</strong> It seems that our classifier - despite our strategy - managed to distinguish almost perfectly between training
and test instances. Something is not right here. Let&rsquo;s take a look at feature importance:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>classifier <span style="color:#f92672">=</span> xgb<span style="color:#f92672">.</span>XGBClassifier(eval_metric<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;logloss&#39;</span>)
</span></span><span style="display:flex;"><span>classifier<span style="color:#f92672">.</span>fit(X, y)
</span></span><span style="display:flex;"><span>fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>,<span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>xgb<span style="color:#f92672">.</span>plot_importance(classifier, ax<span style="color:#f92672">=</span>ax)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show();
</span></span></code></pre></div><p><img loading="lazy" src="../../blog-posts/adversarial-validation/feature-importance-id.png" alt="feature-importance-id"  />
</p>
<p>Something nasty is happening with the <code>Id</code> column - let&rsquo;s go back to the raw data:</p>
<p><img loading="lazy" src="../../blog-posts/adversarial-validation/trainset.png" alt="trainset"  />
</p>
<p><img loading="lazy" src="../../blog-posts/adversarial-validation/testset.png" alt="testset"  />
</p>
<p>It is apparent that our classifier can easily distinguish training from test instances via a very simple
heuristic: <code>Id &gt;= 1461</code>. Totally different distributions between training &amp; test datasets. So, let&rsquo;s drop this column
and try again:</p>
<p><img loading="lazy" src="../../blog-posts/adversarial-validation/roc-auc-curve-2.png" alt="roc-auc-curve-2"  />
</p>
<p>with individual per-fold scores:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;&gt;</span> [<span style="color:#ae81ff">0.5219787952711578</span>,
</span></span><span style="display:flex;"><span>     <span style="color:#ae81ff">0.5326398010883844</span>,
</span></span><span style="display:flex;"><span>     <span style="color:#ae81ff">0.5170529179958716</span>,
</span></span><span style="display:flex;"><span>     <span style="color:#ae81ff">0.5292620566710452</span>,
</span></span><span style="display:flex;"><span>     <span style="color:#ae81ff">0.5295862166360683</span>]
</span></span></code></pre></div><p><strong>Much better.</strong> This is the result we were looking for: our classifier has now lost the ability to distinguish between
training and test datasets yielding a mean ROC AUC score of <code>0.5148</code>.</p>
<p>So, training &amp; test datasets are really drawn from the same underlying distribution - assuming that the highly
problematic<code>Id</code> column is not part of the design matrix.</p>
<h3 id="data-where-the-identically-distributed-property-does-not-hold">data where the &ldquo;identically distributed&rdquo; property does not hold<a hidden class="anchor" aria-hidden="true" href="#data-where-the-identically-distributed-property-does-not-hold">#</a></h3>
<p>The second use case is derived from the
<a href="https://www.kaggle.com/competitions/sberbank-russian-housing-market">Sberbank Russian Housing Market</a> Kaggle
competition.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>trainset <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#34;data/sberbank-russian-housing-market/train.csv&#34;</span>)
</span></span><span style="display:flex;"><span>testset <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#34;data/sberbank-russian-housing-market/test.csv&#34;</span>)
</span></span></code></pre></div><p>Target variable here is <code>price_doc</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X_train <span style="color:#f92672">=</span> trainset<span style="color:#f92672">.</span>drop([<span style="color:#e6db74">&#34;price_doc&#34;</span>], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>Similarly to the <code>Id</code> situation of the previous example, here we have columns <code>id</code> and <code>timestamp</code> that we can drop from
the design matrix, since they are features with highly identifiable properties (having monotonically increasing values).</p>
<p>Then following roughly the same pre-processing logic and applying a 5-fold cross-validation scheme as previously, we
get:</p>
<p><img loading="lazy" src="../../blog-posts/adversarial-validation/roc-auc-curve-3.png" alt="roc-auc-curve-3"  />
</p>
<p>with individual per-fold scores:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;&gt;</span> [<span style="color:#ae81ff">0.9930616257454692</span>,
</span></span><span style="display:flex;"><span>     <span style="color:#ae81ff">0.9933295525996183</span>,
</span></span><span style="display:flex;"><span>     <span style="color:#ae81ff">0.993228891121216</span>,
</span></span><span style="display:flex;"><span>     <span style="color:#ae81ff">0.9927296013456716</span>,
</span></span><span style="display:flex;"><span>     <span style="color:#ae81ff">0.993396535221478</span>]
</span></span></code></pre></div><p>Even with the absence of <code>id</code> and <code>timestamp</code>, our classifier was able to almost perfectly distinguish between training
and test instances. This implies significant difference in the underlying distributions of the two datasets, so using
a validation dataset to derive a proxy performance for the unseen test instances is bound to yield disappointment.</p>
<p>And now, the most important question of all.</p>
<h2 id="is-there-any-hope">is there any hope?<a hidden class="anchor" aria-hidden="true" href="#is-there-any-hope">#</a></h2>
<p>Is there anything we can do when out training and test datasets differ? Can we generate a reasonably good validation
dataset that we can trust?</p>
<p>Intuitively, we cannot do something to &ldquo;bring&rdquo; the training dataset &ldquo;closer&rdquo; to the test dataset. Our best hope is to
select the validation dataset (out of the training dataset) in a way that best resembles the test dataset i.e. select
instances out of the training dataset which are most &ldquo;similar&rdquo; to the test dataset. This solution has been proposed by
Zygmunt Zając (see <em><strong>related reading</strong></em> at the end).</p>
<p>This seems sensible - but how?</p>
<p>Our &ldquo;adversarial&rdquo; classifier can help us here, again. The plan is to:</p>
<ol>
<li>yield predictions for all training instances.</li>
<li>see which training instances are misclassified as test instances.</li>
<li>just use those to form the validation dataset.</li>
</ol>
<p>Regarding step #1, a simple cross-validation scheme is a suitable approach to get predictions for all training
instances.</p>
<p>Now onto step #2: we would like to get all the training instances that were misclassified with high probability i.e. the
classifier mislabelled as test instances (although they were training ones) and it was quite certain about it.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>predictions <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> fold, (train, test) <span style="color:#f92672">in</span> tqdm([<span style="color:#f92672">*</span>enumerate(cv<span style="color:#f92672">.</span>split(X, y))]):
</span></span><span style="display:flex;"><span>    classifier<span style="color:#f92672">.</span>fit(X<span style="color:#f92672">.</span>iloc[train], y<span style="color:#f92672">.</span>iloc[train])
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    y_pred <span style="color:#f92672">=</span> classifier<span style="color:#f92672">.</span>predict_proba(X<span style="color:#f92672">.</span>iloc[test])[:, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    predictions<span style="color:#f92672">.</span>extend(y_pred<span style="color:#f92672">.</span>tolist())
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>combined_dataset[<span style="color:#e6db74">&#34;preds&#34;</span>] <span style="color:#f92672">=</span> predictions
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>trainset <span style="color:#f92672">=</span> combined_dataset[combined_dataset[<span style="color:#e6db74">&#34;_adval_label_&#34;</span>] <span style="color:#f92672">==</span> <span style="color:#ae81ff">0.0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>i <span style="color:#f92672">=</span> trainset[<span style="color:#e6db74">&#34;preds&#34;</span>]<span style="color:#f92672">.</span>argsort()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sorted_trainset <span style="color:#f92672">=</span> trainset<span style="color:#f92672">.</span>iloc[i]
</span></span></code></pre></div><p>That&rsquo;s about it: <code>sorted_trainset</code> contains all the training instances, sorted from lowest to highest probability of
instance being a test instance. &ldquo;Slicing&rdquo; from the bottom of the table will get you the training instances that are
intuitively more similar to the test dataset, thus good candidates for the validation dataset.</p>
<p>For example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>trainset[trainset[<span style="color:#e6db74">&#34;preds&#34;</span>] <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0.5</span>]
</span></span></code></pre></div><p>yields the few misclassified training instances.</p>
<h2 id="a-birds-eye-view">a birds-eye view<a hidden class="anchor" aria-hidden="true" href="#a-birds-eye-view">#</a></h2>
<p>To wrap all these up, let&rsquo;s take a step back and look at the adversarial validation mechanism again - this time in a
bit more abstract way.</p>
<p><img loading="lazy" src="../../blog-posts/adversarial-validation/datasets.png" alt="datasets"  />
</p>
<p>Let&rsquo;s imagine some typical training and test datasets, described by N numerical features and a target variable. The
following flowchart acts as a visual representation of the adversarial validation mechanism.</p>
<p><img loading="lazy" src="../../blog-posts/adversarial-validation/birds-eye-view.png" alt="birds-eye-view"  />
</p>
<h2 id="as-a-python-package">as a Python package<a hidden class="anchor" aria-hidden="true" href="#as-a-python-package">#</a></h2>
<p>Based on the generality of the adversarial validation mechanism above, i searched <a href="https://github.com/">Github.com</a> for
a <code>pip</code> that implements this as a Python package. I didn&rsquo;t find anything worth considering, so i decided to develop and
publish one!</p>
<p>I named it <strong>advertion</strong>, which stands for <strong>adver</strong>sarial valida<strong>tion</strong>):</p>
<p><a href="https://github.com/ilias-ant/adversarial-validation">https://github.com/ilias-ant/adversarial-validation</a></p>
<h2 id="conclusion">conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>We went through the concept of adversarial validation, a simple technique that helps you assert whether training &amp; test
instances follow the same underlying distribution through a classification scheme: membership in test dataset is used as
a binary target variable upon which a classifier tries to distinguish between the two, trained on a combined, shuffled
dataset.</p>
<p>We then saw two practical examples from Kaggle competitions, covering both cases. In the first example, we saw
that the classifier could not distinguish between training &amp; test instances, thus training &amp; test datasets must have the
same distributions. In the second example, we saw quite the opposite.</p>
<p>After that, we tried to answer the most crucial question: what can we do in case our datasets differ? and provided a
simple solution.</p>
<p>Lastly, wrapped up the article with a birds-eye view on the adversarial validation mechanism and highlighted an
open-source Python package that basically offers this validation through a simple API.</p>
<h2 id="related-reading">related reading<a hidden class="anchor" aria-hidden="true" href="#related-reading">#</a></h2>
<p>The following are, to the best of my knowledge, the first mention of the adversarial validation concept and the proposed
solution - definitely a good read:</p>
<ul>
<li><a href="http://fastml.com/adversarial-validation-part-one/">Adversarial validation, part one by Zygmunt Zając</a></li>
<li><a href="http://fastml.com/adversarial-validation-part-two/">Adversarial validation, part two by Zygmunt Zając</a></li>
</ul>
<p>W.r.t. code:</p>
<ul>
<li>Code for the examples above: <a href="https://github.com/ilias-ant/adversarial-validation-demo">https://github.com/ilias-ant/adversarial-validation-demo</a></li>
<li>The open-source Python package: <a href="https://github.com/ilias-ant/adversarial-validation">https://github.com/ilias-ant/adversarial-validation</a></li>
<li>The documentation for the package: <a href="https://advertion.readthedocs.io/en/latest/">https://advertion.readthedocs.io/en/latest/</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/adversarial-validation/">Adversarial-Validation</a></li>
      <li><a href="http://localhost:1313/tags/machine-learning/">Machine-Learning</a></li>
      <li><a href="http://localhost:1313/tags/data-drift/">Data-Drift</a></li>
    </ul>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Adversarial Validation: can i trust my validation dataset? on twitter"
        href="https://twitter.com/intent/tweet/?text=Adversarial%20Validation%3a%20can%20i%20trust%20my%20validation%20dataset%3f&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fadversarial-validation%2f&amp;hashtags=adversarial-validation%2cmachine-learning%2cdata-drift">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Adversarial Validation: can i trust my validation dataset? on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fadversarial-validation%2f&amp;title=Adversarial%20Validation%3a%20can%20i%20trust%20my%20validation%20dataset%3f&amp;summary=Adversarial%20Validation%3a%20can%20i%20trust%20my%20validation%20dataset%3f&amp;source=http%3a%2f%2flocalhost%3a1313%2fblog%2fadversarial-validation%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Adversarial Validation: can i trust my validation dataset? on reddit"
        href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fblog%2fadversarial-validation%2f&title=Adversarial%20Validation%3a%20can%20i%20trust%20my%20validation%20dataset%3f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Adversarial Validation: can i trust my validation dataset? on facebook"
        href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fblog%2fadversarial-validation%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Adversarial Validation: can i trust my validation dataset? on whatsapp"
        href="https://api.whatsapp.com/send?text=Adversarial%20Validation%3a%20can%20i%20trust%20my%20validation%20dataset%3f%20-%20http%3a%2f%2flocalhost%3a1313%2fblog%2fadversarial-validation%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Adversarial Validation: can i trust my validation dataset? on telegram"
        href="https://telegram.me/share/url?text=Adversarial%20Validation%3a%20can%20i%20trust%20my%20validation%20dataset%3f&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fadversarial-validation%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>Copyright © 2025, ARTISAN ML SINGLE MEMBER P.C. -</span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
