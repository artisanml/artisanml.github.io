<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ARTISAN ML</title>
    <link>http://localhost:1313/blog/</link>
    <description>Recent content on ARTISAN ML</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright Â© 2025, ARTISAN ML SINGLE MEMBER P.C. - </copyright>
    <lastBuildDate>Wed, 31 Jan 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Do we even need ML?</title>
      <link>http://localhost:1313/blog/do-we-even-need-ml/</link>
      <pubDate>Wed, 31 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/do-we-even-need-ml/</guid>
      <description>&lt;p&gt;Having worked in the industry for 6 years now, I&amp;rsquo;ve seen machine learning projects succeed but, most
importantly, I&amp;rsquo;ve seen many more fail - I even failed some of them myself due to inexperience and/or poor judgement.
Although each failure has its own story, reasons and learnings, some common denominators always exist.&lt;/p&gt;
&lt;p&gt;One such important denominator can be &amp;ldquo;boiled down&amp;rdquo; to a single assertion that - as an ML engineer - I am particularly
fond of:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Adversarial Validation: can i trust my validation dataset?</title>
      <link>http://localhost:1313/blog/adversarial-validation/</link>
      <pubDate>Sat, 22 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/blog/adversarial-validation/</guid>
      <description>&lt;h2 id=&#34;the-problem&#34;&gt;the problem&lt;/h2&gt;
&lt;p&gt;A common workflow in machine learning projects (especially in &lt;a href=&#34;https://www.kaggle.com/&#34;&gt;Kaggle&lt;/a&gt; competitions) is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;train your ML model in a training dataset.&lt;/li&gt;
&lt;li&gt;tune and validate your ML model in a validation dataset (&lt;em&gt;typically is a discrete fraction of the training
dataset&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;finally, assess the actual generalization ability of your ML model in a &amp;ldquo;held-out&amp;rdquo; test dataset.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This strategy is widely accepted, as it forces the practitioner to  interact with the ever important test dataset
only once, at the end of the model selection process - and purely for performance assessment purposes. Any feedback
derived from the evaluation on the test dataset does not influence the model selection process further, thus preventing
overfitting.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
